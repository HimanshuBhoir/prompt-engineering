## **How LLMs Actually Think**

### **Token prediction (why wording matters)**

Nothing else.

---

## 1ï¸âƒ£ Core Mental Model (this must click)

An LLM does **not**:

* understand meaning
* reason like a human
* know facts

An LLM **only does one thing**:

> **Given previous tokens, predict the most likely next token**

Everything you seeâ€”logic, code, explanationsâ€”is an **emergent effect** of this.

Think of it like this:

```
P(next_token | all_previous_tokens)
```

Thatâ€™s it.

No memory. No intent. No truth-checking.

---

## 2ï¸âƒ£ What is a *token* (important precision)

A token is **not a word**.

Examples:

* `"prompt"` â†’ 1 token
* `"engineering"` â†’ 1 token
* `"ChatGPT"` â†’ 1 token
* `"unbelievable"` â†’ might be `["un", "believable"]`
* `"functionName()"` â†’ multiple tokens

ğŸ‘‰ The model sees **token sequences**, not sentences.

---

## 3ï¸âƒ£ Why wording changes output (deep reason)

Because **small wording changes change token probabilities**.

Example:

### Prompt A

> *Explain closures in JavaScript*

### Prompt B

> *Explain closures in JavaScript simply*

That single word **â€œsimplyâ€** shifts probabilities toward:

* shorter sentences
* fewer technical tokens
* analogy-style patterns

The model isnâ€™t â€œtrying to be simpleâ€.
Itâ€™s following a **token distribution it learned from training data**.

---

## 4ï¸âƒ£ Order matters more than you think

LLMs are **left-to-right** predictors.

This means:

* Earlier tokens influence **everything**
* Late constraints are weaker

### Bad

> Write code. Use TypeScript. Be concise.

### Better

> You are a TypeScript expert.
> Write concise code.

Why?

* â€œTypeScript expertâ€ influences **every following token**
* Late instructions compete with already-formed probability paths

---

## 5ï¸âƒ£ Why vague prompts fail

Example:

> *Write high-quality code*

Problem:

* â€œhigh-qualityâ€ maps to **many token clusters**
* No dominant probability path
* Model picks a generic average

Result:

* Overexplained
* Safe
* Boring
* Often wrong

LLMs prefer **specific, narrow probability spaces**.

---

## 6ï¸âƒ£ Concrete example (token steering)

Prompt 1:

> *Fix this code*

Prompt 2:

> *Apply the minimal change needed to fix this bug.
> Output only the changed lines.*

Prompt 2 works better because:

* â€œminimal changeâ€ suppresses refactor-heavy tokens
* â€œonly changed linesâ€ collapses output space
* Fewer valid next tokens = more control

---

## 7ï¸âƒ£ Key takeaway (memorize this)

> **Prompting is probability steering, not instruction giving**

You are not telling the model *what to do*.
You are **shaping which tokens are likely**.

---

## 8ï¸âƒ£ 5 rules youâ€™ll use everywhere

1. Early tokens matter more than later ones
2. Specific beats vague every time
3. Constraints reduce randomness
4. Output format controls thinking
5. Fewer valid outputs = better results

---

## 9ï¸âƒ£ Micro-exercise (5 minutes, optional)

Take this prompt:

> *Explain React useEffect*

Rewrite it to **strongly bias** toward:

* short answer
* production insight
* no beginner explanation

Do **not** solve it now unless you want feedback.

---

**Answer Prompt**

```
You are a senior React engineer.
Explain Reactâ€™s useEffect from a production perspective.
Focus on real-world pitfalls, dependency management, and performance implications.
Keep the explanation concise and avoid beginner-level definitions.
Limit the response to 5â€“6 bullet points.
```

**Reason**

* â€œSenior React engineerâ€ biases token selection toward expert patterns
* â€œProduction perspectiveâ€ suppresses tutorial-style explanations
* Explicit focus areas narrow the probability space
* â€œAvoid beginner-level definitionsâ€ removes basic conceptual tokens
* Bullet-point and length constraints strongly control verbosity and structure
